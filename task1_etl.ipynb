{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bed1d4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import hashlib\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15b29398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize_value(x):\n",
    "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "        return \"\"\n",
    "    if isinstance(x, (dict, list)):\n",
    "        return json.dumps(x, sort_keys=True, separators=(',', ':'))\n",
    "    return str(x)\n",
    "\n",
    "def row_hash_from_row(row, fields=None):\n",
    "    if fields is None:\n",
    "        fields = [\"track_metadata\", \"listened_at\", \"recording_msid\", \"user_name\"]\n",
    "    parts = [canonicalize_value(row.get(f)) for f in fields]\n",
    "    return hashlib.sha1(\"|\".join(parts).encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e5f25ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      2\u001b[0m     data \u001b[38;5;241m=\u001b[39m [json\u001b[38;5;241m.\u001b[39mloads(line) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file]\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecording_msid\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset.parquet'"
     ]
    }
   ],
   "source": [
    "with open('dataset.parquet') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "df = pd.DataFrame(data).drop(['recording_msid'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ef001ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating primary key 'id' column\n",
    "df[\"id\"] = df.apply(row_hash_from_row, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85544133",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.json_normalize(df['track_metadata'])\n",
    "\n",
    "df_expanded = pd.concat([df.drop(columns=['track_metadata']), meta_df], axis=1)\n",
    "\n",
    "df_expanded.columns = df_expanded.columns.str.replace('^additional_info\\\\.', '', regex=True)                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93a4f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_not_null(val):\n",
    "    return val is not None\n",
    "\n",
    "def val_ms(val):\n",
    "    if pd.isna(val):            \n",
    "        return True\n",
    "    try:\n",
    "        return float(val) <= 600000\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def val_hash(val):\n",
    "    if pd.isna(val):\n",
    "        return True\n",
    "    try:\n",
    "        return isinstance(val, str) and len(val) == 36 and not val.startswith('http')\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def val_date(val):\n",
    "    if pd.isna(val):            \n",
    "        return True\n",
    "    try:\n",
    "        pd.to_datetime(val)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def full_date(val):\n",
    "    if pd.isna(val):\n",
    "        return True\n",
    "    try:\n",
    "        date = pd.to_datetime(val, errors='coerce')\n",
    "        if isinstance(val, str):\n",
    "            date_parts = [p for p in re.split(r'[-/ ,]', val) if p]\n",
    "            return len(date_parts) >= 3 and not pd.isna(date)\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def val_int(val):\n",
    "    if pd.isna(val):\n",
    "        return True\n",
    "    try:\n",
    "        return isinstance(val, int) and val.is_integer()\n",
    "    except Exception:\n",
    "        return False\n",
    "    \n",
    "def val_float(val):\n",
    "    if pd.isna(val):\n",
    "        return True\n",
    "    try:\n",
    "        return isinstance(val, float)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def val_string(val):\n",
    "    if pd.isna(val):\n",
    "        return True\n",
    "    try:\n",
    "        return isinstance(val, str)\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faf00fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_cols = [\"listened_at\",\"recording_msid\",\"user_name\",\"artist_name\",\"track_name\"]\n",
    "list_str_cols = [\"tags\",\"artist_names\",\"release_artist_names\",\"spotify_album_artist_ids\",\"spotify_artist_ids\"]\n",
    "list_hash_cols = [\"work_mbids\",\"artist_mbids\"]\n",
    "date_cols = [\"date\"]\n",
    "int_cols = [\"totaldiscs\",\"totaltracks\",\"tracknumber\",\"discnumber\",\"track_length\",\"duration\"]\n",
    "float_cols = [\"dedup_tag\",\"choosen_by_user\"]\n",
    "hash_cols = [\"recording_msid\",\"release_msid\",\"release_mbid\",\"release_group_mbid\",\"track_mbid\",\"artist_mbid\"]\n",
    "ms_cols = [\"duration_ms\"]\n",
    "string_cols = [c for c in df_expanded.columns if c not in set(non_null_cols + list_str_cols + list_hash_cols + date_cols + int_cols + float_cols + hash_cols + ms_cols)]\n",
    "\n",
    "config = {}\n",
    "for c in non_null_cols: config[c] = val_not_null\n",
    "for c in date_cols: config[c] = val_date\n",
    "for c in int_cols: config[c] = val_int\n",
    "for c in float_cols: config[c] = val_float\n",
    "for c in hash_cols: config[c] = val_hash\n",
    "for c in ms_cols: config[c] = val_ms\n",
    "for c in string_cols: config[c] = val_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68117812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_is_valid(row):\n",
    "    for col, validator in config.items():\n",
    "        val = row.get(col)\n",
    "        if isinstance(val, pd.Series):\n",
    "            # take first non-null scalar\n",
    "            val = next((x for x in val.tolist() if not pd.isna(x)), None)\n",
    "        try:\n",
    "            if not validator(val):\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"Row failed on column {col} with value {val!r}: {e!r}\")\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4203d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid rows: 332218 / 333034\n"
     ]
    }
   ],
   "source": [
    "mask = df_expanded.apply(row_is_valid, axis=1)\n",
    "df_valid = df_expanded[mask].reset_index(drop=True)\n",
    "print(f\"Valid rows: {len(df_valid)} / {len(df_expanded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc26768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid['listened_at'] = pd.to_datetime(df_valid['listened_at'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "664e19bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_valid.drop(['artist_mbids', 'tags', 'work_mbids'], axis=1).dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd9a38d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_duckdb.DuckDBPyConnection at 0x2821f9aaaf0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = duckdb.connect(\"database/sample.db\")\n",
    "con.execute(f\"CREATE SCHEMA IF NOT EXISTS stg;\")                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ecc1311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_duckdb.DuckDBPyConnection at 0x2821f9aaaf0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS stg.raw (\n",
    "    PRIMARY KEY (id),\n",
    "    id VARCHAR NOT NULL,\n",
    "    listened_at TIMESTAMP NOT NULL,\n",
    "    recording_msid VARCHAR NOT NULL,\n",
    "    user_name VARCHAR NOT NULL,\n",
    "    artist_name VARCHAR NOT NULL,\n",
    "    track_name VARCHAR NOT NULL,\n",
    "    release_name VARCHAR,\n",
    "    release_msid VARCHAR,\n",
    "    release_mbid VARCHAR,\n",
    "    recording_mbid VARCHAR,\n",
    "    release_group_mbid VARCHAR,\n",
    "    isrc VARCHAR,\n",
    "    spotify_id VARCHAR,\n",
    "    tracknumber INTEGER,\n",
    "    track_mbid VARCHAR,\n",
    "    artist_msid VARCHAR,\n",
    "    dedup_tag DOUBLE,\n",
    "    artist_names VARCHAR,\n",
    "    discnumber INTEGER,\n",
    "    duration_ms INTEGER,\n",
    "    listening_from VARCHAR,\n",
    "    release_artist_name VARCHAR,\n",
    "    release_artist_names VARCHAR,\n",
    "    spotify_album_artist_ids VARCHAR,\n",
    "    spotify_album_id VARCHAR,\n",
    "    spotify_artist_ids VARCHAR\n",
    ");\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83dd4c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_duckdb.DuckDBPyConnection at 0x2821f9aaaf0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"\"\"\n",
    "            INSERT INTO \n",
    "            stg.raw BY NAME \n",
    "            SELECT * \n",
    "            FROM df_clean\n",
    "            where NOT EXISTS (\n",
    "                SELECT 1 FROM stg.raw r WHERE r.id = df_clean.id\n",
    "            )\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ef5d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
